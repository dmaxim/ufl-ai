{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34LKIju5vzj5"
      },
      "source": [
        "# simulating data for classification\n",
        "\n",
        "To train a neural network for object classification, we need some training data. We've already seen how we can use Scikit-learn to simulate training data for regression problems, but training data for classification problems is different. We need data with one-hot encoded class labels, and it would be nice if the data used for classification was actually correlated with the class labels.\n",
        "\n",
        "Fortunately for us, Scikit-learn has a function, \"make_classification\" that simulates training data for object classification for us! Also fortunately for us, the make_classification function is very similar to the function we used previously for regression problems.\n",
        "\n",
        "The following code cell uses the make_classification function to generate 100 data samples for binary classification.\n",
        "\n",
        "The \"x\" variable holds the data that will be used by the model to infer the class labels, which are typically called \"features\" in object classification. A classification network uses \"feature\" data to infer the \"class label\" of each data sample.\n",
        "\n",
        "The \"y\" variable holds the class labels.\n",
        "\n",
        "In this case, we are simulating data with 10 features (n_features=10) and 2 possible class labels (n_classes=2). As before, we set the random_state option for reproducibility.\n",
        "\n",
        "After simulating the data, we print the simulated features and the class labels, just to have a look at them. We also print the shapes of the feature data and the class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YQ_sV1IEwkD",
        "outputId": "b377a025-9a9e-4d1c-b1eb-afaf768062de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=2,\n",
        "                                           random_state=77317)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7.57559146e-01 -1.39205539e+00 -4.20076974e-01 -5.24730571e-01\n",
            "   1.81116127e-01  1.81826056e+00  2.08134160e+00  4.15560812e-01\n",
            "   2.11673061e-01 -6.70517252e-01]\n",
            " [-7.65224419e-01  1.28937070e-01  1.05532778e+00 -5.67622482e-01\n",
            "  -1.19221293e+00 -1.53555347e+00 -9.70640285e-01  6.34053608e-01\n",
            "  -7.13334685e-01 -1.13011255e+00]\n",
            " [ 7.69469377e-01 -1.53469498e+00 -1.06188509e+00 -4.76155732e-01\n",
            "   3.02061840e-01  1.91583475e+00  1.49728486e+00 -4.32832282e-01\n",
            "   1.41073759e+00 -1.29650603e+00]\n",
            " [-6.09462930e-01  1.90599399e-01 -9.23454222e-01  1.18477119e+00\n",
            "   6.60577592e-02  1.16713464e+00  6.01447236e-01 -6.52514517e-01\n",
            "   5.99669337e-01  2.97395176e-01]\n",
            " [-1.43578796e-01  3.90687988e-01  1.89867973e+00  2.74483811e+00\n",
            "  -2.13208297e+00 -7.94936406e-01  1.01688631e+00  2.22965977e+00\n",
            "  -9.41284508e-01 -8.64584476e-01]\n",
            " [-2.12160407e-02  4.78553711e-01 -4.73623852e-01  5.00375657e-01\n",
            "   8.59861419e-01 -1.27414451e+00 -2.32134300e+00 -1.37100928e+00\n",
            "  -7.77895512e-01  1.22607965e+00]\n",
            " [ 5.48472534e-01 -1.34001788e+00 -1.70831114e-01 -6.69181968e-01\n",
            "   1.59084793e-01  1.07852909e+00  1.32259962e+00  3.56648708e-01\n",
            "   1.97255339e+00  2.65735614e+00]\n",
            " [-5.93031099e-01  1.44204473e+00  1.15750651e+00  2.80605576e-01\n",
            "  -5.89296891e-01 -8.79140092e-01  6.59297820e-02  1.14096561e+00\n",
            "  -9.42989956e-01 -1.25172011e-01]\n",
            " [ 3.01007935e-01  4.46107203e-01 -9.86792124e-01  4.80586842e-01\n",
            "   1.31997684e+00  8.25973527e-01  5.12095889e-02 -9.30360833e-01\n",
            "  -1.75667626e-01  7.63055198e-02]\n",
            " [-2.09425022e+00  1.52388850e+00 -1.46531560e+00  2.94129388e-01\n",
            "  -7.96578253e-01  9.95366706e-01 -2.48543603e-01 -1.50943065e+00\n",
            "   8.86829655e-01  2.16035340e+00]\n",
            " [ 1.24905698e+00  1.60885720e-01  1.79915424e+00  4.77753557e-01\n",
            "  -2.65102388e+00  3.00607286e-01  2.44349111e+00  2.69598050e+00\n",
            "   6.00243198e-01  3.90572101e-01]\n",
            " [-5.44617700e-01  1.96656437e+00  1.07482797e+00 -4.88400026e-01\n",
            "  -3.72816504e-01 -7.83647046e-01  1.07136434e-01  1.07756288e+00\n",
            "  -1.05685315e+00  1.37871791e+00]\n",
            " [ 6.58638803e-01  4.63626087e-01 -1.86989382e+00  4.71021009e-01\n",
            "  -9.30561070e-02 -2.69904120e-01 -2.47985244e+00 -2.77845050e+00\n",
            "   1.37917728e-02  6.81434200e-01]\n",
            " [ 7.24664340e-01  2.22824686e-01 -4.15945634e-01 -6.59847110e-01\n",
            "   1.09877227e+00 -5.19128156e-01 -1.19630587e+00 -8.72100318e-01\n",
            "   6.51129619e-01 -2.01650293e+00]\n",
            " [ 2.74524478e-01  1.46473562e-01  7.16790541e-01  7.59780411e-03\n",
            "   2.14654480e-01 -1.50207098e+00 -1.30397120e+00  1.76595332e-01\n",
            "  -4.93174937e-01 -2.48162849e-01]\n",
            " [ 4.79206773e-02 -1.53613438e+00 -4.54253379e-01 -1.09292338e-02\n",
            "  -5.39799872e-02  1.04793856e+00  9.61215897e-01 -5.87742290e-02\n",
            "   1.38044337e+00 -2.59278036e-01]\n",
            " [ 1.65183551e-01  3.18677692e+00  8.22805439e-01  9.52831756e-01\n",
            "   2.87471184e-01  4.65310954e-01  1.57784255e+00  1.41436797e+00\n",
            "  -8.97869568e-01 -1.01235922e+00]\n",
            " [-1.35403290e+00  2.35661471e-01  1.34732033e+00 -4.59815198e-01\n",
            "  -1.62831731e+00  2.24599927e-01  1.82911898e+00  2.01863616e+00\n",
            "   1.05070480e+00  3.12601246e-01]\n",
            " [ 7.62327792e-01  7.72896719e-01  1.38187893e+00  5.39629250e-01\n",
            "   7.05974815e-01 -7.10124943e-01  5.55354014e-01  1.54996559e+00\n",
            "  -8.27347541e-02  1.60147928e+00]\n",
            " [-8.96174346e-01  2.81125500e-01  1.86585714e+00  3.91682726e-01\n",
            "  -8.85257377e-01 -8.18036655e-01  9.47571274e-01  2.17072758e+00\n",
            "   3.91628709e-01 -4.77881489e-01]\n",
            " [-9.07263141e-01 -1.57920441e+00  1.51369848e-01 -7.01822501e-01\n",
            "   4.85099714e-01 -8.29076179e-01 -9.94169063e-01 -2.45968594e-01\n",
            "   5.43844236e-01 -1.01423065e-01]\n",
            " [-4.08990959e-02  1.09494292e+00  1.10228899e+00  1.35844918e-01\n",
            "   5.63391790e-01 -2.52757749e+00 -2.31093342e+00  1.51112913e-01\n",
            "   5.10527003e-01 -1.92786299e-01]\n",
            " [ 2.40168331e+00 -1.24563052e+00  1.10530964e+00 -7.79035919e-01\n",
            "   1.99748772e+00  1.40822342e+00  3.21932717e+00  2.33336303e+00\n",
            "  -1.67916059e+00  1.05720486e+00]\n",
            " [ 4.66000952e-01 -2.01246982e+00 -1.39939972e+00  2.57434053e+00\n",
            "   2.94877334e-01  1.11498389e+00 -6.51367396e-03 -1.35055819e+00\n",
            "   1.47492478e+00  3.26341273e-01]\n",
            " [-1.60645874e+00 -1.56458844e+00  1.16284498e+00 -7.21857152e-01\n",
            "   5.54096757e-01 -1.31078750e+00 -5.34215319e-01  9.09605454e-01\n",
            "  -9.76549065e-01  5.24991578e-01]\n",
            " [-6.13720990e-01 -1.23327701e+00  6.07461688e-01  4.26255749e-01\n",
            "  -1.05640727e+00 -4.57237205e-01  4.04098166e-02  6.01070466e-01\n",
            "  -8.89919773e-01  2.76403172e-01]\n",
            " [-2.73250124e-01  4.64059034e-02 -9.32030289e-01 -1.00198241e+00\n",
            "   9.82232989e-01  1.75353704e+00  1.41527029e+00 -3.40067773e-01\n",
            "   2.02822995e-01 -4.84780672e-02]\n",
            " [-2.96581430e-02  6.43295237e-01  1.60529907e-01 -7.16388574e-01\n",
            "  -9.46986813e-01  6.44643314e-01  1.08559983e+00  5.82441012e-01\n",
            "   8.98013856e-01 -7.08797278e-01]\n",
            " [ 9.63193592e-02  5.09630270e-01 -1.50019968e+00 -9.87409111e-01\n",
            "  -1.56495227e-02  2.19368729e+00  1.39501119e+00 -8.95348482e-01\n",
            "   7.12905819e-01  6.61339601e-01]\n",
            " [ 2.98799622e-01  4.47755156e-01  2.95417378e-01  1.95190717e+00\n",
            "   7.89502928e-01  6.73108164e-01  1.27711871e+00  7.87846157e-01\n",
            "  -5.23805979e-01 -1.05579624e+00]\n",
            " [ 1.16295324e-01  2.64852301e-01  2.83011931e-02  3.98162305e-01\n",
            "  -7.86454945e-01  1.21071625e+00  1.73195055e+00  7.09780868e-01\n",
            "   2.67409727e-01  8.81237578e-01]\n",
            " [ 7.33544975e-01 -1.21245850e+00  1.35762657e+00 -6.53206397e-01\n",
            "   7.85540868e-01 -8.01366144e-01  3.99980444e-01  1.46537524e+00\n",
            "  -1.12110210e+00  8.28483529e-01]\n",
            " [-1.80689970e+00 -5.28487211e-01  1.18615847e+00 -1.05023430e+00\n",
            "   4.25662260e-01  5.60741634e-01  2.12008111e+00  1.97805580e+00\n",
            "  -2.10599808e+00 -1.50046842e+00]\n",
            " [-1.46323014e+00  1.80950254e+00 -9.19411466e-01  9.04282457e-02\n",
            "   1.05351830e+00  1.39049306e+00  9.19641416e-01 -5.23227731e-01\n",
            "   1.22236376e+00 -1.36502305e+00]\n",
            " [ 2.54703730e-01 -5.15069758e-01 -1.58705938e+00  8.94630375e-01\n",
            "  -3.74143116e-01  1.39491541e+00  1.75744588e-01 -1.45950044e+00\n",
            "   2.21160208e-01  3.36355236e-01]\n",
            " [ 5.52326554e-01  2.04684708e+00 -1.65440427e+00  1.16650135e+00\n",
            "   1.11262153e+00 -1.24979252e-01 -2.03423705e+00 -2.39527106e+00\n",
            "   9.80004931e-01  3.12628237e+00]\n",
            " [-2.35897056e-01  1.30095756e+00  1.30689811e-01  4.59627424e-01\n",
            "   9.44803674e-01 -1.09301783e+00 -1.38804452e+00 -4.21105613e-01\n",
            "   8.05638453e-02 -2.73337964e-01]\n",
            " [ 2.71633732e+00  6.43225640e-01 -6.78765192e-01  6.34719185e-02\n",
            "   1.97648790e-01  9.18406287e-01  5.27079061e-01 -4.46121074e-01\n",
            "   3.01871442e-01 -9.52877993e-01]\n",
            " [-1.61062101e-01  1.12113328e+00 -7.72181757e-02  1.38052985e+00\n",
            "  -7.35585379e-01  9.77088639e-01  1.28532616e+00  4.32134116e-01\n",
            "  -1.32773753e-01  6.45397992e-01]\n",
            " [ 3.06647286e-01  4.93625833e-01 -2.81390562e-01  3.82446690e-01\n",
            "   2.35724546e-01  1.52313933e+00  1.82273135e+00  4.47240458e-01\n",
            "   1.19587010e+00  7.61912008e-02]\n",
            " [ 3.40716210e-01  5.90284615e-01 -3.60434636e-01 -4.46091815e-01\n",
            "   9.26783908e-01 -7.15525592e-01 -1.40973091e+00 -9.02734163e-01\n",
            "  -9.97566640e-01 -1.36328873e+00]\n",
            " [ 1.87795153e+00 -1.34291870e+00 -3.19550043e-01 -2.69408403e+00\n",
            "   1.12859689e+00 -1.68920082e-01 -5.96223445e-01 -5.42768113e-01\n",
            "   2.06642133e-01  3.07443894e-01]\n",
            " [ 8.92092559e-01 -9.33389102e-01  8.43866882e-02  6.11070255e-01\n",
            "  -2.33937490e-01 -2.25094916e+00 -3.06609689e+00 -1.12698714e+00\n",
            "  -3.33226048e-01 -1.28610571e+00]\n",
            " [ 4.03950070e-01 -5.47688821e-01  1.62397864e+00 -3.92821201e-01\n",
            "   8.13773304e-01 -4.96286056e-01  1.12763894e+00  2.00869501e+00\n",
            "  -3.65172484e-01  3.72108143e-01]\n",
            " [-1.15745444e+00 -8.90250588e-01 -8.52504208e-01  5.74477182e-01\n",
            "  -8.94880929e-01  8.55591567e-01  2.43674240e-01 -7.25160513e-01\n",
            "   4.70555278e-01  6.93632167e-01]\n",
            " [ 1.14360713e+00  1.50027056e-01 -1.46569007e-01  5.84803155e-01\n",
            "  -1.45067005e+00  1.34654218e+00  1.72621690e+00  5.39075219e-01\n",
            "  -6.87857906e-01  1.14460376e+00]\n",
            " [ 8.31124404e-01 -4.90642210e-01 -8.40401063e-01 -2.42438720e-01\n",
            "  -8.22705512e-02  9.20515410e-01  3.48441815e-01 -6.72215651e-01\n",
            "   2.33692379e-01 -1.08500145e+00]\n",
            " [ 7.50424092e-01 -1.52561768e-01  7.72858528e-01 -1.95725768e+00\n",
            "  -1.10682790e+00 -8.78280123e-01 -3.65017210e-01  6.00622404e-01\n",
            "   8.95500968e-01  7.98581789e-02]\n",
            " [-9.68884280e-01  1.06219358e+00  8.75343736e-01 -1.40536949e+00\n",
            "   7.15307330e-02 -1.32118329e+00 -8.71823236e-01  4.99622851e-01\n",
            "   4.44953513e-01  3.98466385e-01]\n",
            " [-1.56227524e+00  1.82281398e-01 -3.81053808e-01 -7.09358238e-01\n",
            "  -6.97017203e-01  6.34568171e-01  4.62978804e-01 -1.84606774e-01\n",
            "  -1.91852852e+00  6.47506089e-01]\n",
            " [ 2.90303413e-01  4.23799996e-02  7.50783217e-01 -8.48999262e-01\n",
            "  -9.70656250e-01 -1.18171162e+00 -8.15914066e-01  4.01670631e-01\n",
            "   5.58982782e-01 -8.52810706e-01]\n",
            " [-2.81210359e+00  4.89639188e-01 -1.08960777e+00  1.60740786e-01\n",
            "   7.35289401e-01  1.08917115e+00  3.05292014e-01 -9.29271771e-01\n",
            "   1.83091613e+00  1.63137937e-01]\n",
            " [-1.99973183e-01  3.99870880e-01  1.81655385e+00  7.97583514e-01\n",
            "   3.62585666e-01  1.45080381e-01  2.24464031e+00  2.63437869e+00\n",
            "   5.85674947e-01 -3.36593454e-01]\n",
            " [ 3.13642869e-01 -8.96842462e-01  1.26133093e+00 -1.33130897e+00\n",
            "  -1.36647779e+00 -1.18059814e+00 -2.40746665e-01  1.12012224e+00\n",
            "   1.67844723e+00 -1.24590238e+00]\n",
            " [ 1.45823480e+00 -9.11319163e-01  6.66988847e-01 -3.86453651e-01\n",
            "  -9.94521363e-01 -2.04880007e+00 -2.12767063e+00 -1.95976644e-01\n",
            "   6.91606101e-01  7.44507377e-01]\n",
            " [ 6.67164473e-02 -7.51561413e-01  6.56113171e-01  1.36451522e+00\n",
            "  -2.90630165e-02  9.81531350e-01  2.11546757e+00  1.46566419e+00\n",
            "  -2.40600272e-01 -5.07913648e-01]\n",
            " [ 4.21285850e-01 -1.00859405e+00 -8.15859568e-01 -1.28706595e-01\n",
            "  -5.46672241e-01 -2.98870121e-01 -1.33631349e+00 -1.31249657e+00\n",
            "  -2.19252854e+00  4.82936297e-01]\n",
            " [ 1.28492965e+00 -9.10759837e-01 -1.32471831e-01 -5.06748072e-01\n",
            "   4.59989445e-01  1.11131663e+00  1.41173859e+00  4.28726308e-01\n",
            "   1.01698680e+00 -1.96232230e-01]\n",
            " [ 7.08063534e-01  3.33543718e-02 -6.13394903e-01 -2.04569037e-01\n",
            "   1.93553503e+00 -1.05759332e+00 -2.17428368e+00 -1.44769322e+00\n",
            "  -4.07827345e-01  2.42785487e+00]\n",
            " [ 6.10882022e-01  3.15733556e-01  3.52458934e-01 -8.41486270e-01\n",
            "  -1.21382142e-01 -1.62048564e+00 -1.87958456e+00 -4.01187319e-01\n",
            "   1.88179512e+00 -7.00989229e-01]\n",
            " [-3.81861717e-01  1.16291502e+00 -6.37302446e-01  3.73872534e-01\n",
            "  -5.01240043e-01 -1.00713572e+00 -2.13028865e+00 -1.45338512e+00\n",
            "  -8.50873273e-01  2.82351019e+00]\n",
            " [-1.38943986e+00  5.64534911e-01 -5.67010081e-01 -1.30605410e+00\n",
            "  -1.31625151e+00  7.14636130e-01  3.66491484e-01 -4.01755040e-01\n",
            "  -6.89251358e-01 -4.76310128e-02]\n",
            " [ 3.87887974e-01 -1.70249867e+00 -1.44823414e+00  2.32657410e-02\n",
            "   7.54930406e-02  1.27657459e+00  1.65535030e-01 -1.32979843e+00\n",
            "   1.73777731e-01 -2.18614242e+00]\n",
            " [ 1.12677345e+00 -1.01689730e+00 -1.00699850e+00 -2.41611319e-01\n",
            "  -3.32595163e-01  8.69158098e-01  8.91497284e-02 -9.34873623e-01\n",
            "   9.99013570e-01 -3.74517189e-01]\n",
            " [-4.09761500e-01 -1.24816505e+00  1.88081398e+00 -7.55151086e-01\n",
            "   2.60208207e-01 -1.12755738e+00  5.29729451e-01  2.02047372e+00\n",
            "  -1.35669263e+00 -3.03606044e-01]\n",
            " [ 1.82141250e+00  8.34257176e-01 -2.24960526e+00 -1.48091105e+00\n",
            "  -9.58718922e-01  1.16759249e+00 -8.87849459e-01 -2.51684341e+00\n",
            "  -9.82248431e-01  5.85022984e-01]\n",
            " [ 2.59623025e+00 -4.76793875e-01  1.37479083e+00  1.00091027e-02\n",
            "   4.51126447e-01 -8.88953739e-01  2.96269315e-01  1.44103894e+00\n",
            "  -9.56617078e-01 -8.99007501e-02]\n",
            " [ 1.07074962e+00 -8.45770534e-01  1.05153206e+00 -1.33271767e+00\n",
            "   3.84465755e-01 -6.53392587e-01  2.63873758e-01  1.11688912e+00\n",
            "  -1.26574341e+00 -1.07436669e+00]\n",
            " [-8.87221029e-01 -6.06912091e-01 -1.56907500e+00 -9.87123730e-01\n",
            "  -1.44660764e-01 -5.59951747e-02 -1.84149783e+00 -2.23712263e+00\n",
            "   6.09867918e-01  2.72213587e-02]\n",
            " [-1.50321913e+00  1.96897845e-01 -1.41934398e+00 -1.70123442e+00\n",
            "  -1.55178931e-01  1.11194899e+00 -3.31829512e-02 -1.38027948e+00\n",
            "  -8.34308295e-01  3.70553523e-01]\n",
            " [-8.88345179e-02  1.03763241e+00 -2.30962739e-01 -5.32935698e-01\n",
            "   1.34734059e+00  9.91766792e-01  1.13320497e+00  2.24090282e-01\n",
            "   1.58963086e+00 -4.33356170e-01]\n",
            " [-4.38921790e-01  7.22787065e-01  2.42608109e-01  8.11915864e-01\n",
            "  -8.27348992e-01 -2.00344730e+00 -2.54077841e+00 -7.67562983e-01\n",
            "   1.70555689e+00 -2.39441047e+00]\n",
            " [-7.86133400e-01 -1.49236052e+00  1.17692328e+00  2.83464071e-01\n",
            "   9.12214681e-01 -4.25181133e-01  7.25218640e-01  1.41947869e+00\n",
            "   1.39801106e+00 -7.01310153e-01]\n",
            " [ 4.86391394e-01  8.82416258e-02 -1.25049799e+00 -1.14250143e+00\n",
            "   7.63234726e-01  3.81889600e-01 -8.68672666e-01 -1.54688229e+00\n",
            "   1.08152758e-01 -1.30723489e+00]\n",
            " [ 1.57478605e+00 -7.49102182e-01  1.98924527e+00  3.60471630e-01\n",
            "  -1.13192260e+00 -4.04537327e-01  1.66685704e+00  2.57303585e+00\n",
            "  -8.58400956e-01 -1.58218893e+00]\n",
            " [ 1.53597106e-01 -6.18945767e-01  1.40232336e+00 -9.75548473e-02\n",
            "  -2.17910674e+00 -3.06831288e+00 -2.73317298e+00  2.73730921e-01\n",
            "   2.57485077e+00 -2.46860883e-01]\n",
            " [ 2.08809524e+00 -4.26581955e-01 -1.30707634e+00  2.07502034e-01\n",
            "   1.87942827e-03 -3.76973051e-01 -1.99787526e+00 -2.04637329e+00\n",
            "  -6.91595330e-02 -8.84834223e-01]\n",
            " [-2.56911154e-01 -1.73975008e+00 -7.37110310e-02 -3.58810101e-01\n",
            "   1.01463483e-01  1.21175225e+00  1.61879393e+00  5.66923916e-01\n",
            "  -4.25357046e-01  3.29710690e-01]\n",
            " [ 4.12028058e-01 -2.62601268e-01  4.57990989e-01  1.78974702e-01\n",
            "  -1.08175343e-01 -2.12897337e+00 -2.47506486e+00 -5.34196315e-01\n",
            "  -5.99411122e-01 -2.52457537e-01]\n",
            " [-1.37767194e+00 -3.48391157e-01  2.04662604e+00  5.27717149e-01\n",
            "  -1.41104352e+00 -1.96347531e+00 -4.57821236e-01  1.79102494e+00\n",
            "  -8.20117306e-02 -1.40084637e+00]\n",
            " [-1.20738396e+00 -1.58436126e+00  5.46083332e-01  2.35276677e+00\n",
            "   5.49043761e-01  1.02709971e+00  2.05583789e+00  1.33617782e+00\n",
            "  -4.40350169e-01  3.86737707e-01]\n",
            " [-1.17182322e+00  1.78919699e+00 -1.35913884e+00 -1.06435525e+00\n",
            "  -2.02564723e+00  1.03866181e+00 -6.84559420e-02 -1.33618626e+00\n",
            "  -1.81249125e-02  8.07439300e-01]\n",
            " [ 6.47092643e-01 -4.32702703e-01  4.85755240e-01  6.93567647e-01\n",
            "  -6.54929519e-01 -2.29379995e+00 -2.67533004e+00 -5.86371622e-01\n",
            "  -1.12452503e+00 -8.09302329e-01]\n",
            " [-1.58388340e-01 -9.14199690e-01 -6.51984331e-01  1.14873661e+00\n",
            "   2.97182623e-02  6.61147262e-01  1.95910347e-01 -5.50829526e-01\n",
            "   1.85002817e-01 -4.63076210e-02]\n",
            " [ 7.95597781e-01  9.83382038e-02 -7.63493083e-01  7.21828132e-01\n",
            "  -8.16989468e-02 -3.27993887e-01 -1.31837658e+00 -1.25498533e+00\n",
            "  -1.85061891e-01 -6.38236667e-01]\n",
            " [ 1.78611832e+00 -3.56957003e-03 -2.40500673e+00  1.61785257e-01\n",
            "  -8.37763006e-02  1.04916269e+00 -1.22874966e+00 -2.80087648e+00\n",
            "   5.75114259e-01  7.57389367e-01]\n",
            " [ 3.40420087e-01 -1.60707387e-01 -3.14720214e-01 -1.46240839e+00\n",
            "   2.43324450e+00 -1.03062365e-01 -4.98316093e-01 -4.99532821e-01\n",
            "  -1.07738983e-01 -7.93789635e-01]\n",
            " [ 7.94608193e-01  1.50294912e+00 -3.48701651e-02 -9.98361823e-01\n",
            "  -9.09914157e-01 -1.73147231e+00 -2.47060497e+00 -1.00719416e+00\n",
            "   2.08847467e+00 -1.74682707e+00]\n",
            " [-5.26257137e-01  1.15877228e+00 -6.85078644e-01 -5.62110758e-01\n",
            "   1.66621706e+00  8.29411607e-01  3.95014658e-01 -5.04245945e-01\n",
            "   8.77205013e-02 -1.23433918e+00]\n",
            " [ 2.97322950e+00 -2.38464209e+00 -8.56214346e-01  4.81431909e-01\n",
            "  -1.97833640e+00  9.94025857e-01  4.33902875e-01 -6.53769903e-01\n",
            "  -4.57751407e-01  1.21713512e+00]\n",
            " [ 6.36430752e-01  1.44198888e-01 -1.26446979e+00  9.86411166e-01\n",
            "  -1.49251591e+00 -1.57695162e-01 -1.64208451e+00 -1.86512351e+00\n",
            "  -6.39044022e-01 -1.92900332e+00]\n",
            " [-5.48307387e-01  4.67231245e-01 -1.47950909e+00 -4.43961953e-01\n",
            "  -7.35115931e-01  1.11284836e+00 -9.95159240e-02 -1.46437456e+00\n",
            "  -6.85935356e-02  7.39980593e-01]\n",
            " [ 1.15296484e+00  1.09557160e+00 -8.38790574e-01 -1.89330102e+00\n",
            "  -7.49896231e-02 -9.20692921e-01 -2.23527421e+00 -1.68884362e+00\n",
            "  -2.49799053e-02  1.32617638e+00]\n",
            " [ 8.52760416e-01  6.48771962e-01  5.12858357e-01 -2.46419551e-01\n",
            "  -5.76412241e-01 -2.02697921e+00 -2.27019523e+00 -4.00610450e-01\n",
            "  -3.17692280e-01  2.38189825e-01]\n",
            " [ 1.49955274e+00  1.47210647e-01 -1.12861639e+00  2.62050879e-01\n",
            "  -7.47221634e-01  1.11163426e+00  2.93009490e-01 -9.71687607e-01\n",
            "   6.04859108e-01 -6.89465414e-01]\n",
            " [-8.81728639e-02  1.33233872e+00 -1.46117457e+00 -7.27588778e-01\n",
            "  -1.21121261e+00  9.59998667e-01 -2.93556862e-01 -1.52318037e+00\n",
            "   7.17879058e-01 -1.73055504e+00]\n",
            " [ 1.59590480e+00 -1.06588035e+00 -1.14231472e+00 -9.62341928e-01\n",
            "  -4.08992662e-01  9.84400684e-01  9.89509780e-02 -1.06135643e+00\n",
            "  -7.79487580e-01  4.06730548e-01]\n",
            " [ 6.17911210e-01 -4.62383894e-01  6.84098436e-01  1.41383119e+00\n",
            "  -1.50492570e-01  7.56221474e-01  1.83051675e+00  1.38032924e+00\n",
            "  -5.26076065e-01 -5.01879121e-01]\n",
            " [ 4.45153428e-01  5.45492122e-01 -6.16735194e-01 -2.91950231e-01\n",
            "  -7.14626226e-01  7.71955935e-01  3.91116588e-01 -4.39949298e-01\n",
            "  -1.31095512e+00  3.62689492e-01]\n",
            " [ 2.67896344e-01 -2.14065848e+00  1.35087690e+00 -1.13818450e+00\n",
            "   7.50146584e-01 -7.84613176e-01  4.15922578e-01  1.46515593e+00\n",
            "   2.42328541e-02 -7.47124375e-01]]\n",
            "[1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1]\n",
            "(100, 10) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7_PjrIlJAur"
      },
      "source": [
        "There is quite a bit of feature data in the output; 100 data samples of 10 features each, to be precise. You can see that the features are floating-point numbers.\n",
        "\n",
        "The binary class labels (there are 100 of them, of course, one for each data sample) are either 0 or 1.\n",
        "\n",
        "So, it looks like we have some data we can use for binary classification. Of course, we'll need to package these data into a tensorflow Dataset object to train our network, but we can deal with that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4U3ymBGExIw"
      },
      "source": [
        "# binary classification\n",
        "\n",
        "To do binary classification, we need a neural network. We are free to build any network we want, but the input and output shapes of the network must match the shape of the features and the shape of the class labels, respectively.\n",
        "\n",
        "Let's look at the data to determine the input and output shapes required.\n",
        "\n",
        "The feature data \"x\" has shape (100,10). There are 100 data samples (we can ignore this, it will be handled by the batch dimension in tensorflow), and there are 10 features per data sample. So, our feature data are 10-dimensional, and we'll need our network to be able to handle input vectors (rank-1 tensors) in 10 dimensions. This can be done by specifying:\n",
        "\n",
        "    input_shape=[10]\n",
        "\n",
        "in the first layer of our network.\n",
        "\n",
        "The class label data \"y\" has shape (100,); it is basically scalar data, which can be treated as a rank-1 tensor (aka, vector) of 1 dimension in tensorflow. This means we will need a single neuron in our network's output layer.\n",
        "\n",
        "For a binary classification problem, we'll need to specify sigmoid activation for the output layer. In tensorflow, sigmoid activation is implemented using a tf.keras.activations.sigmoid object, so we can specify sigmoid activation in tensorflow using the option:\n",
        "\n",
        "    activation=tf.keras.activations.sigmoid\n",
        "\n",
        "when we create the network's output layer.\n",
        "\n",
        "We'll also need to specify cross-entropy loss when we compile our model. Tensorflow implements cross-entropy loss for binary classification using a tf.keras.losses.BinaryCrossentropy object, so we can specify binary cross-entropy loss using the option:\n",
        "\n",
        "    loss=tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "Pretty simple so far. We can use the same optimizer for classification problems as we did for regression problems. And batch training is the same whether the problem is regression or classification.\n",
        "\n",
        "For classification problems, we often want to know how often our model predicts the correct class label during the training process. We can see this information by adding the option:\n",
        "\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "when we call model.fit(...). With this option set, tensorflow will report the proportion of training samples correctly classified at each step in the training process. Ideally, we'd like model accuracy to be very close to 1.0 by the end of the training process.\n",
        "\n",
        "Using this information, let's build a simple *linear* classifier for our data in tensorflow.\n",
        "\n",
        "We'll include the data simulation code in the following code cell, just for completeness. We then need to package the data into a tensorflow Dataset object, build our neural network classifier, compile the network using the appropriate loss function, and train the network. In this case, we won't worry about splitting the data into training and validation subsets, although in practice we typically would."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZB-Ic_OE1WA",
        "outputId": "f4721811-1fa9-4aa1-ff9c-0b628a591399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=2,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=[10], activation=tf.keras.activations.sigmoid))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=100)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11 (44.00 Byte)\n",
            "Trainable params: 11 (44.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 6ms/step - loss: 0.4843 - binary_accuracy: 0.7900\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4689 - binary_accuracy: 0.7900\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4549 - binary_accuracy: 0.7900\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4422 - binary_accuracy: 0.8000\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4307 - binary_accuracy: 0.8000\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.4201 - binary_accuracy: 0.8100\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.4103 - binary_accuracy: 0.8100\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.4014 - binary_accuracy: 0.8100\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3932 - binary_accuracy: 0.8100\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3856 - binary_accuracy: 0.8100\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3785 - binary_accuracy: 0.8100\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3720 - binary_accuracy: 0.8200\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3659 - binary_accuracy: 0.8300\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.3602 - binary_accuracy: 0.8300\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3549 - binary_accuracy: 0.8300\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3500 - binary_accuracy: 0.8400\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3453 - binary_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3410 - binary_accuracy: 0.8500\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3368 - binary_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3330 - binary_accuracy: 0.8500\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.3293 - binary_accuracy: 0.8700\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.3259 - binary_accuracy: 0.8700\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.3226 - binary_accuracy: 0.8700\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3195 - binary_accuracy: 0.8700\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3166 - binary_accuracy: 0.8700\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3138 - binary_accuracy: 0.8800\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.3111 - binary_accuracy: 0.8800\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.3086 - binary_accuracy: 0.8800\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3062 - binary_accuracy: 0.8800\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3039 - binary_accuracy: 0.8800\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3017 - binary_accuracy: 0.8800\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2995 - binary_accuracy: 0.8900\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2975 - binary_accuracy: 0.8900\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2956 - binary_accuracy: 0.8900\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2937 - binary_accuracy: 0.8900\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2920 - binary_accuracy: 0.9000\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2903 - binary_accuracy: 0.9000\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2886 - binary_accuracy: 0.9000\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2870 - binary_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2855 - binary_accuracy: 0.9000\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2840 - binary_accuracy: 0.9000\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2826 - binary_accuracy: 0.9000\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2813 - binary_accuracy: 0.9000\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2799 - binary_accuracy: 0.9000\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2787 - binary_accuracy: 0.9000\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2774 - binary_accuracy: 0.9000\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2763 - binary_accuracy: 0.9000\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2751 - binary_accuracy: 0.9000\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2740 - binary_accuracy: 0.9000\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2729 - binary_accuracy: 0.9000\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2719 - binary_accuracy: 0.9000\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2709 - binary_accuracy: 0.9000\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2699 - binary_accuracy: 0.9000\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2689 - binary_accuracy: 0.9000\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2680 - binary_accuracy: 0.9000\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2671 - binary_accuracy: 0.9000\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2662 - binary_accuracy: 0.9000\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.2654 - binary_accuracy: 0.9000\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2646 - binary_accuracy: 0.9000\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2638 - binary_accuracy: 0.9000\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2630 - binary_accuracy: 0.9000\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.2622 - binary_accuracy: 0.9000\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2615 - binary_accuracy: 0.9000\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2608 - binary_accuracy: 0.8900\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2601 - binary_accuracy: 0.8900\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2594 - binary_accuracy: 0.8900\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.2587 - binary_accuracy: 0.8900\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2581 - binary_accuracy: 0.8900\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2575 - binary_accuracy: 0.8900\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2568 - binary_accuracy: 0.8900\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2562 - binary_accuracy: 0.8900\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2557 - binary_accuracy: 0.8900\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2551 - binary_accuracy: 0.8900\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2545 - binary_accuracy: 0.8900\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2540 - binary_accuracy: 0.8900\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2534 - binary_accuracy: 0.8900\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2529 - binary_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2524 - binary_accuracy: 0.8900\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2519 - binary_accuracy: 0.8900\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2514 - binary_accuracy: 0.8900\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.2509 - binary_accuracy: 0.8900\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2505 - binary_accuracy: 0.8900\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2500 - binary_accuracy: 0.8900\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2496 - binary_accuracy: 0.8900\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.2491 - binary_accuracy: 0.8900\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2487 - binary_accuracy: 0.8900\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2483 - binary_accuracy: 0.8900\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2479 - binary_accuracy: 0.8900\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2475 - binary_accuracy: 0.8900\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2471 - binary_accuracy: 0.9000\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2467 - binary_accuracy: 0.9000\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2463 - binary_accuracy: 0.9000\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2459 - binary_accuracy: 0.9000\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2456 - binary_accuracy: 0.9000\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2452 - binary_accuracy: 0.9000\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2449 - binary_accuracy: 0.9000\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2445 - binary_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2442 - binary_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2439 - binary_accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2435 - binary_accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e4fad037430>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQy7NxXRRXI"
      },
      "source": [
        "From the model summary (scroll back up through the training output), you can see that the linear classifier has 11 trainable parameters: a weight for each of the 10 input features, plus the bias term.\n",
        "\n",
        "The model's loss is high when training starts, and accuracy is pretty poor, but the loss and accuracy both improve pretty quickly. By the end of the training process, our linear classifier achieves ~0.9 accuracy, indicating that it can correctly classify approximately 90/100 data samples.\n",
        "\n",
        "Not too shabby for a simple linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuT93h7yE22e"
      },
      "source": [
        "# simulating multi-label classification data\n",
        "\n",
        "Let's extend our binary classification problem to the case in which there is more than two possible classes.\n",
        "\n",
        "It's relatively easy to use scikit-learn's make_classification function to simulate multi-label classification data; all you need to do is set the n_classes option to the number of classes you'd like to simulate.\n",
        "\n",
        "In the following code cell, edit the FIXME part to simulate 3 possible classes. You'll notice we added an option:\n",
        "\n",
        "    n_clusters_per_class=1\n",
        "\n",
        "This is just to make the math calculations work in scikit-learn for 3-class data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_cAaMuKE59c",
        "outputId": "47fe0e36-fc4d-43eb-ef4b-cc27ecc6db49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=3,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.42844947e-01  1.18096857e+00  6.68894191e-01 -1.22003570e+00\n",
            "  -1.93489730e-01 -1.98203719e+00 -3.45016405e-01 -1.86087719e+00\n",
            "  -1.78017154e-03 -1.42344667e+00]\n",
            " [ 7.59104610e-01 -3.50580424e-01  1.74343097e+00 -8.61200413e-02\n",
            "   8.72065118e-01  5.25993321e-01 -1.10604243e+00  6.94870418e-01\n",
            "   1.65651437e+00 -1.26861650e-01]\n",
            " [-3.70635637e-01  6.33235268e-01 -1.45131713e+00 -8.23165713e-01\n",
            "  -6.64237313e-01 -1.02634305e+00  5.20527783e-01 -1.08096943e+00\n",
            "   2.37772891e+00  2.84170533e-01]\n",
            " [ 6.43574063e-01 -2.44149585e+00 -7.73487541e-01  4.36401703e-01\n",
            "  -2.46025965e-01  4.04574749e+00 -2.91044603e-01  3.96550578e+00\n",
            "  -9.08913649e-01 -6.15959202e-01]\n",
            " [ 8.67635728e-01  9.94516423e-02 -1.42399968e+00  4.90803959e-01\n",
            "  -4.61037642e-01 -1.71599731e-01 -1.19867961e-01 -1.46002987e-01\n",
            "   4.62340832e-01  1.01118479e+00]\n",
            " [ 5.50575404e-01  5.77431631e-01  5.96240774e-01  3.41686560e-01\n",
            "   6.91106050e-01 -9.12356298e-01  9.30619157e-01 -1.03945816e+00\n",
            "  -6.97377256e-03  1.36580007e+00]\n",
            " [ 7.90481340e-01  1.76572621e+00  8.41646203e-01  1.12577623e+00\n",
            "   2.20503380e-01 -3.10373404e+00 -3.23314208e+00 -2.46197855e+00\n",
            "  -1.79543680e+00 -9.36756587e-02]\n",
            " [-8.41917940e-02  2.09762813e-01  8.79711767e-01  8.72209007e-01\n",
            "  -5.43228314e-01 -4.18971752e-01 -1.35753873e+00 -1.77725185e-01\n",
            "   7.13226878e-01  9.37941109e-02]\n",
            " [ 1.90605562e-01  9.85937484e-01 -1.60227048e-01  1.17240412e+00\n",
            "   2.99125784e-01 -1.72422976e+00 -1.63450973e+00 -1.39484085e+00\n",
            "  -9.47025266e-01  9.35260346e-01]\n",
            " [ 8.80125419e-01  4.25135745e-01 -2.14333713e+00  3.25965559e-01\n",
            "  -7.09859198e-01 -7.40465745e-01 -6.46280520e-01 -6.08352906e-01\n",
            "  -1.61018032e+00  8.18908529e-01]\n",
            " [ 2.41931354e-02 -3.55368416e-01  4.97505448e-01  3.57273328e-01\n",
            "   1.17632131e+00  5.34328716e-01 -1.09883991e+00  7.01730816e-01\n",
            "   1.65642226e-01  1.28649621e+00]\n",
            " [ 1.30885373e-01  3.55325524e-01 -1.68093555e+00  7.30881708e-01\n",
            "   9.54207461e-01 -5.16367611e-01  1.44535006e+00 -7.42508298e-01\n",
            "  -1.16397021e+00  4.05789292e-01]\n",
            " [ 5.84463573e-01  7.32139818e-01  1.65427852e-02  6.34960579e-01\n",
            "  -9.61918322e-01 -1.25826070e+00 -7.85273761e-01 -1.08629391e+00\n",
            "  -1.26946031e+00 -2.47763674e-01]\n",
            " [ 3.22934065e-01 -1.10384015e-01  5.51057569e-01  8.61282798e-02\n",
            "  -6.14921218e-01  3.46345832e-01  3.15236588e+00 -1.93864373e-01\n",
            "  -1.58929889e-01 -1.72314834e-01]\n",
            " [-2.18703303e+00 -2.16494880e-01 -1.16034116e+00  6.76516271e-02\n",
            "  -1.53552058e+00  3.01060565e-01 -1.14318105e+00  4.83349211e-01\n",
            "   1.48144205e+00  5.35545770e-01]\n",
            " [ 1.07050595e-01  7.31040310e-01 -1.08601052e+00 -3.84419845e-01\n",
            "   1.00302760e-02 -1.18459979e+00  6.06056644e-01 -1.24853326e+00\n",
            "   1.73891267e-01 -5.27137577e-01]\n",
            " [-5.27630960e-01  2.50049969e-01  8.57132919e-01  1.03180518e+00\n",
            "  -3.51932021e-02 -2.83155774e-01  2.57097422e+00 -7.05686215e-01\n",
            "   7.20516651e-01 -7.99724123e-01]\n",
            " [ 8.97656918e-01  5.87782532e-01  1.03037754e+00  1.16946386e+00\n",
            "  -1.65480560e-01 -9.26721113e-01  9.85843266e-01 -1.06263457e+00\n",
            "  -1.27018138e+00 -1.78939430e-02]\n",
            " [ 5.63682589e-01  5.93067204e-01 -4.45091834e-01 -1.17110420e-01\n",
            "   4.09551635e-02 -1.04104453e+00 -1.05825973e+00 -8.30185300e-01\n",
            "   2.08805216e-01 -4.07711101e-01]\n",
            " [ 9.81604268e-01  6.12441806e-01 -9.79800650e-01 -2.98799749e-02\n",
            "   2.21565026e+00 -1.03732829e+00 -3.62118006e-01 -9.43442539e-01\n",
            "  -6.22579717e-01  9.00037485e-01]\n",
            " [ 3.55094496e-01  7.89442736e-01  7.25795094e-01  1.17341576e-01\n",
            "  -8.36095080e-01 -1.31127648e+00  3.38927425e-02 -1.27512392e+00\n",
            "  -1.63332077e+00 -4.56095800e-01]\n",
            " [-4.72611493e-01 -1.79045934e+00 -3.25021629e-01  8.21842687e-01\n",
            "  -2.83755875e-01  2.95054355e+00 -5.30821041e-01  2.94549801e+00\n",
            "  -2.65168990e-01  2.72763833e-01]\n",
            " [ 2.26498270e-01 -5.89137995e-01  5.48538544e-01 -1.04701990e+00\n",
            "  -8.03000679e-01  9.26570953e-01 -1.03241860e+00  1.07030737e+00\n",
            "  -9.30993053e-01  3.00422551e-01]\n",
            " [-1.09119554e+00 -5.82623794e-01  8.99559065e-01 -1.14777338e+00\n",
            "  -3.98086446e-01  9.16366500e-01 -1.02021266e+00  1.05837964e+00\n",
            "  -1.90990208e-01 -1.02456296e+00]\n",
            " [ 4.55583116e-01  5.44348427e-01 -1.46336157e-02 -5.10034574e-01\n",
            "  -7.20340103e-01 -8.33655917e-01  1.38919121e+00 -1.04024531e+00\n",
            "  -5.86305109e-01 -3.06489412e-02]\n",
            " [-1.74567786e+00 -2.53626387e+00  3.05844980e-01  7.93372493e-01\n",
            "  -6.97460957e-02  4.20600008e+00 -2.40076615e-01  4.11208930e+00\n",
            "  -5.34049963e-01  9.08365618e-01]\n",
            " [-1.30634316e-01  8.85699827e-01 -3.95321474e-01 -4.35179181e-01\n",
            "  -2.34408200e+00 -1.44412683e+00  5.61654680e-01 -1.49232564e+00\n",
            "  -1.55249168e-01 -1.27730003e+00]\n",
            " [ 1.85342182e-01 -3.85949456e-02  1.66431851e+00 -1.11799821e-01\n",
            "   7.03067102e-01  1.12906784e-03 -1.22148408e+00  2.06132304e-01\n",
            "   1.31860677e+00  2.71688932e-01]\n",
            " [-1.41603385e+00  4.39828902e-01 -1.42308186e+00  6.80026687e-01\n",
            "  -1.02262457e+00 -8.11663096e-01 -1.55196847e+00 -5.25248923e-01\n",
            "   1.21304496e+00 -2.39514873e-01]\n",
            " [ 1.33889432e+00  1.54488238e-01  2.18171474e-01 -1.20445142e+00\n",
            "   9.26413280e-01 -2.47105936e-01  1.90666776e-01 -2.71226449e-01\n",
            "  -1.22221038e+00 -1.47878352e+00]\n",
            " [ 1.54162824e+00  1.10071043e+00 -2.47497417e+00 -2.97755927e-01\n",
            "  -2.40739198e-01 -1.90558117e+00 -1.44967713e+00 -1.60143162e+00\n",
            "   1.65491665e+00  6.89900798e-02]\n",
            " [ 5.20023575e-02 -4.98889714e-01  6.95804061e-01  4.48076972e-01\n",
            "   2.74663958e-01  7.74071972e-01 -1.07881102e+00  9.30461984e-01\n",
            "  -1.25243648e-01 -8.93725527e-01]\n",
            " [ 4.72255148e-02  1.25147760e-01 -5.82443469e-01 -7.19979369e-01\n",
            "   1.09063042e+00 -2.40168499e-01 -6.20177181e-01 -1.28401486e-01\n",
            "  -2.89922740e-01 -9.27187487e-01]\n",
            " [ 5.97771688e-01 -2.03951758e-02 -7.57324494e-01  6.12860751e-01\n",
            "   5.47055372e-01 -3.00409498e-02 -1.23890823e+00  1.78881727e-01\n",
            "  -3.98994495e-01  1.16815997e-01]\n",
            " [ 1.04110917e+00  4.34246544e-02  6.91881731e-01  1.43022196e+00\n",
            "  -1.08251623e+00 -5.51759057e-02  3.30232217e-01 -1.08848321e-01\n",
            "   1.27281799e+00  2.70517328e-02]\n",
            " [-5.39536177e-01  8.18868789e-01  3.51538650e-02  1.15086745e+00\n",
            "  -1.24315947e+00 -1.37515905e+00 -2.55487209e-01 -1.28839250e+00\n",
            "  -5.49691272e-01 -2.83908389e-01]\n",
            " [-9.02366755e-01  1.28123537e+00  1.15539420e-01  8.94841434e-01\n",
            "   6.97694335e-01 -2.24765472e+00 -2.25967181e+00 -1.79662353e+00\n",
            "  -7.44297040e-02 -2.05973726e-01]\n",
            " [-8.23525158e-01 -1.42704659e+00  7.13868096e-01 -1.68684367e+00\n",
            "   4.55505447e-01  2.34048018e+00 -6.39748050e-01  2.37318586e+00\n",
            "  -4.34964937e-01  1.70185946e+00]\n",
            " [-9.16722464e-01  6.61617874e-01  1.53596422e-02  3.11764516e-01\n",
            "  -3.01776025e-01 -1.07098973e+00  5.70116318e-01 -1.13251544e+00\n",
            "  -6.03154326e-01  9.71956519e-01]\n",
            " [ 1.15711106e+00  4.16073418e-01 -6.93132744e-01 -2.42932585e-01\n",
            "  -5.41947772e-01 -6.14988260e-01  1.49216892e+00 -8.45841088e-01\n",
            "  -3.78515478e-01  3.40935316e-01]\n",
            " [-3.25687426e-01  2.46620561e-01  7.29538763e-01 -5.71909885e-01\n",
            "   7.38763570e-01 -2.90712505e-01  2.31412663e+00 -6.69887181e-01\n",
            "  -7.44617359e-01  8.59470860e-01]\n",
            " [-4.88000358e-01  9.66226856e-01 -4.09313246e-01  6.97960511e-03\n",
            "  -7.41987835e-01 -1.62927350e+00 -4.30271190e-01 -1.50505898e+00\n",
            "  -1.55756139e+00 -2.03577220e+00]\n",
            " [ 5.05704545e-01 -3.84281101e-01 -4.40136663e-01  2.32614940e-01\n",
            "  -1.71000409e+00  5.83427060e-01 -1.07928005e+00  7.45979138e-01\n",
            "   7.33665233e-02  8.16278715e-01]\n",
            " [-2.58044175e-01  9.25387164e-01  1.29023425e-01  1.29480768e+00\n",
            "  -1.00634322e+00 -1.58604440e+00 -9.08627632e-01 -1.38291201e+00\n",
            "   6.88426983e-01  2.05877335e+00]\n",
            " [-6.11320454e-02 -1.19185133e+00  3.25845868e-01 -2.05173001e-01\n",
            "  -7.89825908e-01  1.94236699e+00 -7.73955822e-01  2.01030438e+00\n",
            "  -1.74769725e-01 -1.88175916e+00]\n",
            " [-2.13416929e-01  8.98066566e-01 -1.75506417e+00  3.10607633e-01\n",
            "   8.29399115e-01 -1.59224799e+00 -1.90893116e+00 -1.22100592e+00\n",
            "  -5.54329468e-01  1.24337658e+00]\n",
            " [-8.31711623e-01  1.93014254e-01  1.73761875e+00 -7.15269891e-01\n",
            "  -1.54143252e-01 -3.88780233e-01 -1.31231591e+00 -1.56088190e-01\n",
            "   1.13773347e-01 -2.07116907e+00]\n",
            " [ 3.90750696e-01  1.25229723e-01 -1.63524754e-01  3.89975344e-02\n",
            "   2.00143632e-01 -2.51625399e-01 -8.39447492e-01 -1.02685921e-01\n",
            "  -1.46800237e+00  9.38865709e-01]\n",
            " [-7.26741193e-01  6.75066839e-01 -1.20466082e+00  1.83735107e-01\n",
            "   8.59759718e-01 -1.18586685e+00 -1.22169523e+00 -9.42952042e-01\n",
            "   5.16437315e-01 -3.61965502e-01]\n",
            " [-8.83810694e-01  5.22608345e-01  1.55092081e+00 -4.29222842e-01\n",
            "   1.14089141e-01 -8.07210226e-01  1.20105628e+00 -9.83063007e-01\n",
            "   1.19254329e+00 -5.01276886e-01]\n",
            " [-4.33006340e-02 -3.00610204e-01 -1.52776647e+00 -8.05420092e-01\n",
            "   6.04230982e-01  4.41801795e-01 -1.12695078e+00  6.16875069e-01\n",
            "  -6.38977861e-01  9.52496799e-01]\n",
            " [ 3.75377177e-01  3.55143735e-01  1.85550004e+00  2.73139264e-02\n",
            "   8.08961378e-01 -6.45308801e-01 -1.05799178e+00 -4.47122176e-01\n",
            "   1.41286097e+00 -3.89876247e-01]\n",
            " [ 3.20834674e-01 -1.23380123e+00 -4.33093740e-01  1.78559873e+00\n",
            "   1.90455305e+00  2.01492093e+00 -7.20079715e-01  2.07149950e+00\n",
            "   1.87758619e+00 -3.14343741e-01]\n",
            " [ 1.21157576e+00  6.87385235e-01 -1.69086770e+00 -3.02853515e-01\n",
            "   8.78581750e-01 -1.11023819e+00  6.40012606e-01 -1.18224435e+00\n",
            "   5.70741472e-01 -1.07149345e+00]\n",
            " [ 2.62015179e+00  7.55894654e-01  2.53607857e+00  5.96335393e-02\n",
            "   1.20646364e+00 -1.22044310e+00  7.12494726e-01 -1.30109962e+00\n",
            "   3.66849577e-01  4.55012596e-02]\n",
            " [ 9.61171509e-02  9.35813029e-01  2.83849743e-01  4.79875522e-01\n",
            "  -4.74583421e-01 -1.57067661e+00 -2.75092230e-01 -1.47438034e+00\n",
            "  -4.00893788e-02  7.37367821e-01]\n",
            " [-1.72913189e-01  8.96098567e-01 -9.82314020e-02  1.28020871e-02\n",
            "   2.03223968e-01 -1.56629393e+00 -1.46962227e+00 -1.26962277e+00\n",
            "   4.61184403e-02 -5.95521731e-01]\n",
            " [-4.82256748e-01  9.89889134e-02  1.90928000e+00  1.76168837e+00\n",
            "   5.66774086e-01 -2.31471303e-01 -1.29443837e+00 -6.79980428e-03\n",
            "   1.05891126e+00  5.93046225e-01]\n",
            " [-1.20083160e+00  8.59448226e-01 -1.39007966e+00  5.70199715e-01\n",
            "  -1.23705287e+00 -1.42671041e+00  5.32895440e-02 -1.39013041e+00\n",
            "   1.33259193e-02  2.16293876e-02]\n",
            " [ 5.67569751e-01  1.01597239e-01 -1.36349111e-01  7.30396048e-01\n",
            "   2.18145567e+00 -2.36465641e-01 -1.30714627e+00 -9.50161946e-03\n",
            "   2.30676739e+00  8.66560616e-01]\n",
            " [ 1.42476635e+00 -3.94895042e-01 -1.92498821e+00  1.32509509e-01\n",
            "  -2.20609488e+00  6.01809247e-01 -1.06516490e+00  7.61405381e-01\n",
            "   5.83856758e-01  6.32481556e-01]\n",
            " [ 2.50764545e-01 -9.22241068e-01 -6.56795165e-01 -6.87382252e-01\n",
            "   4.70516587e-01  1.48839815e+00 -8.81347608e-01  1.58884820e+00\n",
            "   1.09460938e+00 -2.68343609e-01]\n",
            " [-4.00689890e-01  7.76567087e-01  1.25171442e+00 -7.95563877e-01\n",
            "  -1.02347333e+00 -1.25585210e+00  6.92622860e-01 -1.33204304e+00\n",
            "   5.55540895e-01  3.09333839e-01]\n",
            " [-1.96528241e+00  7.62743145e-01  4.16197204e-01  2.35208766e-01\n",
            "  -5.92027780e-02 -1.36665103e+00 -1.89880617e+00 -1.00430717e+00\n",
            "  -1.17688279e+00  3.29715920e-01]\n",
            " [ 4.19515163e-01 -4.80012477e-01  1.24026351e+00 -1.50518872e+00\n",
            "  -1.24737938e+00  7.42843563e-01 -1.07554121e+00  8.99681178e-01\n",
            "   1.09762456e+00  6.10883432e-01]\n",
            " [-3.49349770e-01  7.96331568e-01 -1.43348379e+00  3.24144475e-01\n",
            "   6.49211337e-01 -1.28326844e+00  7.98312287e-01 -1.37632565e+00\n",
            "  -4.61801133e-01  9.12661910e-01]\n",
            " [-2.31517231e-01  5.17287508e-01  2.41586351e-01 -1.23773279e+00\n",
            "  -1.10416516e+00 -9.36692055e-01 -1.47831400e+00 -6.58652001e-01\n",
            "  -2.33793884e+00 -2.57140029e-01]\n",
            " [-1.24057412e+00 -8.21659137e-01 -3.25461304e-01  1.42904606e+00\n",
            "  -5.83458967e-01  1.32008076e+00 -9.01226524e-01  1.42923858e+00\n",
            "  -1.33198899e-01  2.46098535e-01]\n",
            " [-3.48631144e-01 -1.37046728e-01  2.45360621e-01  4.35912813e-01\n",
            "  -6.76319097e-01  2.40219468e-01  2.37833910e-01  1.92631266e-01\n",
            "  -1.84337110e+00 -1.94899243e-01]\n",
            " [-1.91736672e+00  6.21093798e-01  6.90441135e-01 -1.04525634e+00\n",
            "  -1.39060567e+00 -1.09151637e+00 -1.13296890e+00 -8.66505884e-01\n",
            "  -7.01624283e-01 -2.24582899e-01]\n",
            " [-7.50693455e-01  4.63677030e-01  7.94044591e-01 -4.46157131e-01\n",
            "   8.84291130e-01 -6.91488908e-01  1.54398526e+00 -9.28598591e-01\n",
            "  -1.11184070e+00 -1.10751376e+00]\n",
            " [ 5.76033741e-01 -7.56592223e-01  3.74687501e-01  4.40288227e-01\n",
            "  -2.03213383e+00  1.20969329e+00 -9.43182202e-01  1.32941621e+00\n",
            "   1.59632970e-02  7.38683546e-01]\n",
            " [ 3.70778755e-01  4.80297277e-01  9.53552315e-01  1.02964196e+00\n",
            "  -1.11845594e-01 -7.22611534e-01  1.47659406e+00 -9.47415779e-01\n",
            "  -1.07861986e+00 -1.42272822e-01]\n",
            " [ 6.57350064e-01  1.12233886e+00  1.68809906e+00  1.65840449e-01\n",
            "  -6.56693917e-01 -1.94223834e+00 -1.46292708e+00 -1.63469494e+00\n",
            "   7.91602116e-01  8.15178381e-01]\n",
            " [-4.83710960e-01 -9.52380279e-01 -6.26189569e-02  1.47994033e+00\n",
            "  -1.44989747e+00  1.53918309e+00 -8.68632748e-01  1.63587831e+00\n",
            "   1.32230855e+00 -4.79412964e-01]\n",
            " [ 3.25161730e-01 -1.22475218e+00  5.57824567e-01  6.99684715e-01\n",
            "   1.34907386e-01  1.99735308e+00 -7.68835670e-01  2.06267643e+00\n",
            "  -1.35173803e+00  2.61138507e-01]\n",
            " [ 2.05267875e-01  6.91638931e-02  1.82125059e+00  2.84152063e-01\n",
            "   2.49357319e-01  2.77053358e-02  2.76477504e+00 -4.37275998e-01\n",
            "  -6.55628245e-01  1.48302667e-01]\n",
            " [-6.16262905e-01  1.92637278e-01 -5.62893700e-01 -1.02910777e+00\n",
            "   1.44757942e+00 -2.93469200e-01  5.21636425e-01 -3.71667118e-01\n",
            "   1.14291782e+00 -7.18433744e-01]\n",
            " [-4.89042913e-01  8.61492580e-01 -8.46077390e-01 -1.59428457e+00\n",
            "   3.72274857e-01 -1.52503175e+00 -1.78525464e+00 -1.17669494e+00\n",
            "  -2.45206660e-01  9.91401361e-01]\n",
            " [ 5.33633562e-01  8.99805358e-02  2.73328742e-01  9.95300745e-02\n",
            "  -1.01587860e+00 -2.07089606e-01 -1.11239371e+00 -1.37542677e-02\n",
            "  -2.86173688e-01  6.12613758e-01]\n",
            " [ 3.74090754e-01 -3.79349939e-01 -4.34995263e-01 -9.79005205e-01\n",
            "  -3.55692331e-01  5.73853244e-01 -1.10585745e+00  7.41172118e-01\n",
            "  -4.36985188e-01  4.08462207e-01]\n",
            " [-1.76219308e+00  2.33328944e-01 -9.86287919e-01 -2.47588090e+00\n",
            "   9.75050175e-01 -4.41094082e-01 -1.02683689e+00 -2.54653475e-01\n",
            "   9.03746603e-01 -1.08489410e+00]\n",
            " [ 3.92693284e-01  7.10360176e-01 -6.85324169e-01  1.83265960e+00\n",
            "   1.97002541e+00 -1.14980207e+00  6.13840350e-01 -1.21615249e+00\n",
            "  -1.00186523e+00  1.16012745e-01]\n",
            " [ 7.62393107e-01  3.79730381e-01  4.62305283e-01  1.73269125e+00\n",
            "  -8.33072452e-01 -6.55588058e-01 -4.65022957e-01 -5.56609635e-01\n",
            "   8.66691002e-01  5.64788467e-01]\n",
            " [ 2.06067079e-01  3.30358654e-01  5.66193418e-01  9.58563529e-01\n",
            "  -9.99973197e-01 -6.21354640e-01 -1.39248159e+00 -3.67784654e-01\n",
            "   1.03065749e+00  6.83607191e-02]\n",
            " [-5.78173540e-01  7.25444029e-01  1.78081316e+00 -1.21610410e+00\n",
            "  -7.63500634e-02 -1.19085492e+00  3.04613174e-01 -1.20398826e+00\n",
            "   4.03891275e-01  6.44719788e-01]\n",
            " [-2.02573524e+00  4.95374768e-01 -7.61919536e-01 -5.93649956e-01\n",
            "   5.08149261e-01 -7.45884438e-01  1.51154494e+00 -9.75812945e-01\n",
            "   3.08484060e-01 -2.57762333e+00]\n",
            " [-6.50141684e-01  4.32654987e-01 -3.13592167e-01 -8.35537033e-01\n",
            "   6.66759604e-01 -7.65573044e-01 -8.90352676e-01 -5.91688909e-01\n",
            "   9.48322083e-01  2.34908043e-02]\n",
            " [ 9.16145497e-01  2.29007508e-01  5.75705885e-01  1.41069391e+00\n",
            "   2.38080084e+00 -2.82024385e-01  1.91499614e+00 -5.94477939e-01\n",
            "  -1.25544151e-01 -4.62829357e-02]\n",
            " [ 8.94489564e-01  7.64232603e-01 -1.88286728e-01  4.67557587e-01\n",
            "  -1.21473111e+00 -1.23945050e+00  6.12947592e-01 -1.30279045e+00\n",
            "   6.29776523e-01 -3.48102072e-01]\n",
            " [-5.42432492e-01  1.07133593e+00  7.64749563e-01 -9.54509466e-01\n",
            "  -4.06533351e-01 -1.90590835e+00 -2.40232687e+00 -1.44183584e+00\n",
            "  -9.64076861e-01  2.80360179e+00]\n",
            " [ 7.34551003e-01  9.33480862e-01 -4.99615900e-01 -1.47748181e+00\n",
            "  -7.93609607e-01 -1.55930576e+00 -1.29979845e-01 -1.48773099e+00\n",
            "   4.72071569e-01 -3.41332026e-01]\n",
            " [ 1.84269214e-01  4.33034270e-01  2.84057426e-01  3.45293519e-01\n",
            "   1.23528998e+00 -6.38547565e-01  1.58224578e+00 -8.83769005e-01\n",
            "  -5.99805803e-01  1.13618071e+00]\n",
            " [-4.72309914e-01  2.64323887e-01  2.66610150e-02  2.17942349e+00\n",
            "   7.79693715e-01 -5.10471785e-01 -1.37211049e+00 -2.63859495e-01\n",
            "   5.43491804e-01 -5.34547293e-01]\n",
            " [-3.31936193e-01 -1.12828936e+00 -7.60859615e-01 -2.04164257e+00\n",
            "  -6.62911875e-01  1.83520255e+00 -8.01966636e-01  1.91126139e+00\n",
            "  -1.65958420e-01 -8.05185145e-01]\n",
            " [-4.11569899e-02 -1.16376371e+00  8.43825453e-01 -2.29500586e-01\n",
            "  -8.70797159e-01  1.89309480e+00 -8.23463012e-01  1.97091474e+00\n",
            "   3.85977993e-01  1.27400876e+00]\n",
            " [ 1.12126153e+00  8.34356920e-01 -1.91237423e+00 -2.58017800e-01\n",
            "  -8.41642946e-01 -1.43323804e+00 -8.81472714e-01 -1.23953982e+00\n",
            "  -1.63816864e-01 -3.04036021e-01]\n",
            " [ 9.56547583e-01  6.47209457e-01 -2.94101243e+00  9.07960368e-01\n",
            "   9.56912383e-02 -1.11799147e+00 -8.04442626e-01 -9.47282866e-01\n",
            "  -2.69180407e-01 -4.86067990e-02]\n",
            " [ 1.08374615e+00 -1.51333951e+00 -1.28147881e+00 -1.11922522e+00\n",
            "   2.36174394e-01  2.48185257e+00 -6.81444590e-01  2.51704639e+00\n",
            "   9.24273244e-01 -8.00568231e-01]\n",
            " [ 1.71305643e+00  7.68534366e-01  5.88613111e-01  5.34286605e-01\n",
            "  -7.70796721e-01 -1.24247610e+00  6.92927492e-01 -1.31914499e+00\n",
            "   2.10184662e+00  4.04089740e-02]]\n",
            "[2 1 2 1 0 2 0 1 0 0 1 2 0 2 1 2 2 2 0 0 2 1 1 1 2 1 2 1 0 0 0 1 0 1 0 0 0\n",
            " 1 2 2 2 2 1 0 1 0 1 0 0 2 1 0 1 2 2 2 0 1 2 1 1 1 2 0 1 2 1 2 0 0 2 1 2 0\n",
            " 1 1 1 0 0 0 1 0 2 0 2 2 2 0 2 2 0 2 2 1 1 1 0 0 1 2]\n",
            "(100, 10) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZVXdUUVg7lk"
      },
      "source": [
        "If everything works out, you should see similar feature data (x) as before. The class label data (y) might look a bit strange, though!\n",
        "\n",
        "Notice that the class labels appear to be integers: 0, 1, 2. These are definitely *not* one-hot encoded!\n",
        "\n",
        "Not to worry, we'll deal with one-hot encoding in tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNvQoaMYE7MI"
      },
      "source": [
        "# multi-label classification\n",
        "\n",
        "Now that we have some 3-class data, we need to build a neural network classifier to classify it.\n",
        "\n",
        "Notice that the shape of the feature data (x) is the same as before, so we can use the same input_shape option for our network's input layer.\n",
        "\n",
        "The class label data (y) needs some special attention. Although the shape of the class label data is the same as before, there are now 3 possible classes, rather than 2. *If* the class label data were one-hot encoded, we'd need 3 output neurons, one for each of the possible classes. The *actual* class label data are *not* one-hot encoded, but we're going to allow tensorflow to internally *convert* the data to one-hot encoding, so we need to design our network's output layer *as if* the data were one-hot encoded. That means we need 3 neurons in the output layer.\n",
        "\n",
        "We'll need to use softmax activation in the output layer, as well. Tensorflow implements softmax activation as the object: tf.keras.activations.softmax, so we can specify:\n",
        "\n",
        "    activation=tf.keras.activations.softmax\n",
        "\n",
        "when we create the output layer, in order to use softmax activation.\n",
        "\n",
        "We'll use the loss function to calculate cross-entropy loss *and* convert the class labels to one-hot encoding, all in one step.\n",
        "\n",
        "To do this, we'll use a tf.keras.losses.SparseCategoricalCrossentropy object. This object automatically converts the integer-encoded class labels (y) to one-hot encoding (\"Sparse\") and calculates multi-label cross-entropy loss (\"CategoricalCrossentropy\"). We'll just need to specify:\n",
        "\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "when we compile the model.\n",
        "\n",
        "Other than those changes, the rest of the model construction and training is pretty much the same. We do have to change the accuracy metric from tf.keras.metrics.BinaryAccuracy to tf.keras.metrics.SparseCategoricalAccuracy, to handle >2 possible classes (see line #22).\n",
        "\n",
        "The following code cell implements multi-label data simulation, builds a model, compiles it, and executes batch training. Edit the FIXME portions to simulate 3-class data (line #7), use softmax activation (line #16) and train using sparse categorical cross-entropy loss (line #21)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCGdoschE-ou",
        "outputId": "3d7b42e8-186d-4627-9057-efcc9bffbf03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=3,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=3, input_shape=[10], activation=tf.keras.activations.softmax))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33 (132.00 Byte)\n",
            "Trainable params: 33 (132.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.9071 - sparse_categorical_accuracy: 0.5600\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8690 - sparse_categorical_accuracy: 0.5900\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8355 - sparse_categorical_accuracy: 0.6000\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8058 - sparse_categorical_accuracy: 0.6500\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7793 - sparse_categorical_accuracy: 0.6500\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7557 - sparse_categorical_accuracy: 0.7000\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7344 - sparse_categorical_accuracy: 0.7000\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7152 - sparse_categorical_accuracy: 0.7100\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6977 - sparse_categorical_accuracy: 0.7200\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6819 - sparse_categorical_accuracy: 0.7300\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6673 - sparse_categorical_accuracy: 0.7500\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6540 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6418 - sparse_categorical_accuracy: 0.7800\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6305 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6200 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6104 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6014 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5930 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5852 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5779 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5711 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5647 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5587 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5531 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5478 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5428 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5381 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5337 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5295 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5255 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5218 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5182 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5149 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5117 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5086 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5057 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5030 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5004 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4979 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4955 - sparse_categorical_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4933 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4911 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4890 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4870 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4851 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4833 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4816 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4799 - sparse_categorical_accuracy: 0.8400\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4783 - sparse_categorical_accuracy: 0.8400\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4768 - sparse_categorical_accuracy: 0.8400\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4753 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4738 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4725 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4712 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4699 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4687 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4675 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4663 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4652 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4642 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4631 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4621 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4612 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4602 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4593 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4585 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4576 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4568 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4560 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4552 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4545 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4538 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4530 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4524 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4517 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4510 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4504 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4498 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4492 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4486 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4480 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4475 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4464 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4459 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4454 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4449 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4444 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4440 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4435 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4431 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4426 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4422 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4418 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4414 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4410 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4406 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4402 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4398 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4395 - sparse_categorical_accuracy: 0.8800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e4f9d623970>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30l8zpanE_N9"
      },
      "source": [
        "The model now has 33 trainable parameters, 11 for *each* of the 3 output neurons. When I ran the previous code cell, the model's loss stabilized to around 0.44, and the accuracy plateaued at around 0.87. Not too bad.\n",
        "\n",
        "Let's see if we can 'improve' our model's accuracy on the training data by adding a few more layers *before* the output layer.\n",
        "\n",
        "Edit the following code cell to include 2 new Dense neural-network layers, each with 8 units and ReLU activation. Remember that you'll need to specify the input_shape of the *first* layer in the network. Notice that we've also increased the training run to 300 epochs, in order to better fit the model's additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p99ZhF1tHskO",
        "outputId": "ab64cd75-6fee-4335-b200-33c852d6ce52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "# simulate training data\n",
        "x,y = sklearn.datasets.make_classification(n_samples=100,\n",
        "                                           n_features=10,\n",
        "                                           n_classes=3,\n",
        "                                           n_clusters_per_class=1,\n",
        "                                           random_state=77317)\n",
        "\n",
        "# package training data into tensorflow Dataset\n",
        "data = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "\n",
        "# create and summarize linear neural network classifier\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=8, activation=tf.keras.activations.relu, input_shape=[10]))\n",
        "model.add(tf.keras.layers.Dense(units=8, activation=tf.keras.activations.relu))\n",
        "model.add(tf.keras.layers.Dense(units=3, activation=tf.keras.activations.softmax))\n",
        "model.summary()\n",
        "\n",
        "# compile model with loss function and optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# batch data and train model\n",
        "data = data.batch(10)\n",
        "model.fit(data, epochs=500)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 8)                 88        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 187 (748.00 Byte)\n",
            "Trainable params: 187 (748.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 1.1170 - sparse_categorical_accuracy: 0.2800\n",
            "Epoch 2/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0945 - sparse_categorical_accuracy: 0.3400\n",
            "Epoch 3/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0740 - sparse_categorical_accuracy: 0.3700\n",
            "Epoch 4/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0549 - sparse_categorical_accuracy: 0.4000\n",
            "Epoch 5/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0373 - sparse_categorical_accuracy: 0.4200\n",
            "Epoch 6/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.0208 - sparse_categorical_accuracy: 0.4200\n",
            "Epoch 7/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0049 - sparse_categorical_accuracy: 0.4500\n",
            "Epoch 8/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9897 - sparse_categorical_accuracy: 0.4500\n",
            "Epoch 9/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9751 - sparse_categorical_accuracy: 0.5100\n",
            "Epoch 10/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9612 - sparse_categorical_accuracy: 0.5200\n",
            "Epoch 11/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9480 - sparse_categorical_accuracy: 0.5400\n",
            "Epoch 12/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9352 - sparse_categorical_accuracy: 0.5500\n",
            "Epoch 13/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9228 - sparse_categorical_accuracy: 0.5500\n",
            "Epoch 14/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.9108 - sparse_categorical_accuracy: 0.5600\n",
            "Epoch 15/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8993 - sparse_categorical_accuracy: 0.5800\n",
            "Epoch 16/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8882 - sparse_categorical_accuracy: 0.5800\n",
            "Epoch 17/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8775 - sparse_categorical_accuracy: 0.6000\n",
            "Epoch 18/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8670 - sparse_categorical_accuracy: 0.6100\n",
            "Epoch 19/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8568 - sparse_categorical_accuracy: 0.6200\n",
            "Epoch 20/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8470 - sparse_categorical_accuracy: 0.6400\n",
            "Epoch 21/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8375 - sparse_categorical_accuracy: 0.6400\n",
            "Epoch 22/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8281 - sparse_categorical_accuracy: 0.6500\n",
            "Epoch 23/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.8189 - sparse_categorical_accuracy: 0.6500\n",
            "Epoch 24/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8099 - sparse_categorical_accuracy: 0.6800\n",
            "Epoch 25/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8012 - sparse_categorical_accuracy: 0.6800\n",
            "Epoch 26/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7928 - sparse_categorical_accuracy: 0.7000\n",
            "Epoch 27/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7846 - sparse_categorical_accuracy: 0.7200\n",
            "Epoch 28/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7766 - sparse_categorical_accuracy: 0.7200\n",
            "Epoch 29/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7688 - sparse_categorical_accuracy: 0.7400\n",
            "Epoch 30/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7611 - sparse_categorical_accuracy: 0.7500\n",
            "Epoch 31/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7534 - sparse_categorical_accuracy: 0.7600\n",
            "Epoch 32/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7460 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 33/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7388 - sparse_categorical_accuracy: 0.7800\n",
            "Epoch 34/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7316 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 35/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7246 - sparse_categorical_accuracy: 0.7700\n",
            "Epoch 36/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7177 - sparse_categorical_accuracy: 0.7800\n",
            "Epoch 37/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7108 - sparse_categorical_accuracy: 0.7900\n",
            "Epoch 38/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7040 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 39/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6973 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 40/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6908 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 41/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6843 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 42/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6780 - sparse_categorical_accuracy: 0.8000\n",
            "Epoch 43/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 44/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6656 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 45/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6596 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 46/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6536 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 47/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6477 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 48/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6420 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 49/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6364 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 50/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6308 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 51/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6254 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 52/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6200 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 53/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6148 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 54/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6096 - sparse_categorical_accuracy: 0.8300\n",
            "Epoch 55/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6047 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 56/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5997 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 57/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5948 - sparse_categorical_accuracy: 0.8500\n",
            "Epoch 58/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5900 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 59/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5852 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 60/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 61/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5759 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 62/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5711 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 63/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5663 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 64/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5616 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 65/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5570 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 66/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5524 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 67/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5479 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 68/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5434 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 69/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5388 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 70/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5343 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 71/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5298 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 72/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5254 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 73/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5211 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 74/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.5169 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 75/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5128 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 76/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5086 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 77/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5045 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 78/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5004 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 79/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4964 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 80/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 81/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4884 - sparse_categorical_accuracy: 0.8700\n",
            "Epoch 82/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4843 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 83/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4804 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 84/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4764 - sparse_categorical_accuracy: 0.8800\n",
            "Epoch 85/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4726 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 86/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4688 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 87/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4652 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 88/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4614 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 89/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4577 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 90/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4540 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 91/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4504 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 92/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4470 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 93/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4435 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 94/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4401 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 95/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4368 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 96/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4335 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 97/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4303 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 98/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4271 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 99/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4240 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 100/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4209 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 101/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4179 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 102/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4149 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 103/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4120 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 104/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4091 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 105/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4063 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 106/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4035 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 107/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4007 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 108/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3980 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 109/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3953 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 110/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 111/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3901 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 112/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3877 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 113/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3852 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 114/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3829 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 115/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3806 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 116/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3783 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 117/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3760 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 118/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3738 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 119/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3716 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 120/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3695 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 121/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3673 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 122/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3653 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 123/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3632 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 124/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3611 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 125/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3592 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 126/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 127/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3553 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 128/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3534 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 129/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3516 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 130/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3497 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 131/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3479 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 132/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3460 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 133/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3441 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 134/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3422 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 135/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3403 - sparse_categorical_accuracy: 0.8900\n",
            "Epoch 136/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3385 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 137/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3367 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 138/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3349 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 139/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3331 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 140/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3313 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 141/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3296 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 142/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3279 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 143/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3261 - sparse_categorical_accuracy: 0.9000\n",
            "Epoch 144/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3245 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 145/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3228 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 146/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3211 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 147/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3194 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 148/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3177 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 149/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3161 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 150/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 151/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 152/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3112 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 153/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3096 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 154/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3080 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 155/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3064 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 156/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3048 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 157/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3034 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 158/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3019 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 159/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3003 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 160/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2989 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 161/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2974 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 162/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2958 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 163/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2944 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 164/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2929 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 165/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2915 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 166/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2900 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 167/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2885 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 168/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2871 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 169/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2857 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 170/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2842 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 171/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 172/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2814 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 173/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2800 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 174/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2786 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 175/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2772 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 176/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 177/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2744 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 178/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2731 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 179/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2717 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 180/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2703 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 181/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2690 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 182/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2676 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 183/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2663 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 184/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2649 - sparse_categorical_accuracy: 0.9200\n",
            "Epoch 185/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2636 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 186/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 187/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2610 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 188/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2596 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 189/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2584 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 190/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2571 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 191/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2558 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 192/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2545 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 193/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2533 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 194/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2520 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 195/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 196/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 197/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2483 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 198/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2471 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 199/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2459 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 200/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2447 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 201/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2436 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 202/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2423 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 203/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2412 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 204/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2400 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 205/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2389 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 206/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2377 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 207/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2366 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 208/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2354 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 209/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2342 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 210/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2331 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 211/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2320 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 212/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2309 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 213/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2298 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 214/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 215/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2276 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 216/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2265 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 217/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2254 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 218/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2243 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 219/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2233 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 220/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2223 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 221/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2212 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 222/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2201 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 223/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2191 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 224/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2181 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 225/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2170 - sparse_categorical_accuracy: 0.9300\n",
            "Epoch 226/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2160 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 227/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2150 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 228/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2140 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 229/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2130 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 230/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2120 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 231/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2110 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 232/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2100 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 233/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2090 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 234/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2080 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 235/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2071 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 236/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2062 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 237/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2052 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 238/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2043 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 239/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2033 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 240/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2023 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 241/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2014 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 242/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2004 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 243/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1995 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 244/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1986 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 245/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1976 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 246/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1968 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 247/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1959 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 248/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1949 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 249/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1941 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 250/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1932 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 251/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1922 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 252/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1914 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 253/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 254/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 255/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1888 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 256/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1879 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 257/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1870 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 258/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1861 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 259/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1853 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 260/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1844 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 261/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1835 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 262/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1827 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 263/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1818 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 264/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 265/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1801 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 266/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 267/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1785 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 268/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1776 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 269/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1767 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 270/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1759 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 271/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1751 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 272/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1742 - sparse_categorical_accuracy: 0.9400\n",
            "Epoch 273/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1734 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 274/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1726 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 275/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1719 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 276/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1711 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 277/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1703 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 278/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1696 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 279/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1688 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 280/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1680 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 281/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1673 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 282/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1665 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 283/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1658 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 284/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 285/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1642 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 286/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1635 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 287/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1627 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 288/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1620 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 289/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1612 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 290/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1605 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 291/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1597 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 292/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1590 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 293/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1582 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 294/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1575 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 295/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1568 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 296/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1561 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 297/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1553 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 298/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 299/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1539 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 300/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1531 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 301/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1524 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 302/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1517 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 303/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1510 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 304/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1503 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 305/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1497 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 306/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1489 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 307/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 308/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1475 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 309/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1469 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 310/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1462 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 311/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1455 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 312/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1449 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 313/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1442 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 314/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1436 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 315/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1429 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 316/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1422 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 317/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1416 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 318/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1409 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 319/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1403 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 320/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1397 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 321/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1390 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 322/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 323/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1378 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 324/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1371 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 325/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1365 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 326/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1358 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 327/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1352 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 328/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1346 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 329/500\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1340 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 330/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1334 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 331/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1328 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 332/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1322 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 333/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1316 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 334/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1309 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 335/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1304 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 336/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 337/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 338/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1285 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 339/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1279 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 340/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 341/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1268 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 342/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 343/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1258 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 344/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 345/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1247 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 346/500\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1241 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 347/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 348/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1231 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 349/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1225 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 350/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1220 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 351/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1216 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 352/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1210 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 353/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1205 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 354/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1199 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 355/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1195 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 356/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 357/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1184 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 358/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1179 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 359/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1175 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 360/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 361/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 362/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1160 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 363/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1154 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 364/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1149 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 365/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 366/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 367/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1135 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 368/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1130 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 369/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1126 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 370/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 371/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1115 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 372/500\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1111 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 373/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1106 - sparse_categorical_accuracy: 0.9600\n",
            "Epoch 374/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1102 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 375/500\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1097 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 376/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1092 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 377/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1088 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 378/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1083 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 379/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1079 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 380/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1073 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 381/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1069 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 382/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1065 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 383/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 384/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1056 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 385/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1051 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 386/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1047 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 387/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1042 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 388/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1038 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 389/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1034 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 390/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 391/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 392/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1021 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 393/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1016 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 394/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 395/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1007 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 396/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 397/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0999 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 398/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0995 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 399/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0990 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 400/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0986 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 401/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0982 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 402/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0978 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 403/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 404/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0969 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 405/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0965 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 406/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0960 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 407/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0956 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 408/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 409/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0948 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 410/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0944 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 411/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0940 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 412/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0936 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 413/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 414/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0928 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 415/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0924 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 416/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 417/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0916 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 418/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 419/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0909 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 420/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 421/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0900 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 422/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0896 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 423/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 424/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0889 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 425/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 426/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0880 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 427/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0876 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 428/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0873 - sparse_categorical_accuracy: 0.9700\n",
            "Epoch 429/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0869 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 430/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0864 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 431/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0861 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 432/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 433/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0853 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 434/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0851 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 435/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0847 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 436/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 437/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0841 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 438/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0838 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 439/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0834 - sparse_categorical_accuracy: 0.9800\n",
            "Epoch 440/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0832 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 441/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 442/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0825 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 443/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0822 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 444/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0819 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 445/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0816 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 446/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 447/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 448/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0807 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 449/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0802 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 450/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0801 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 451/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0797 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 452/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 453/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0792 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 454/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0788 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 455/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 456/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0783 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 457/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0779 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 458/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0777 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 459/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0773 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 460/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 461/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0768 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 462/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0765 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 463/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0762 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 464/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 465/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0756 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 466/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 467/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 468/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 469/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 470/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0742 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 471/500\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0740 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 472/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 473/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0735 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 474/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0732 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 475/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0729 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 476/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0727 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 477/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 478/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0722 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 479/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0718 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 480/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0716 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 481/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0714 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 482/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0711 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 483/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 484/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 485/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 486/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 487/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0698 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 488/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0696 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 489/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0693 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 490/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 491/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0688 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 492/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0685 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 493/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 494/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0681 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 495/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0679 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 496/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 497/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 498/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 499/500\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0669 - sparse_categorical_accuracy: 0.9900\n",
            "Epoch 500/500\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e4f98fb5450>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Rutz6nJ9AZ"
      },
      "source": [
        "This model has 187 trainable parameters, more than the number of data samples used to train the model! This should make us highly suspicious that our model might be overfitting the training data!\n",
        "\n",
        "Nonetheless, this model should achieve fairly high accuracy on the training data.\n",
        "\n",
        "After completing the quiz, I'll leave it as a do-on-your-own exercise to:\n",
        "\n",
        "1. simulate a larger data set, and see if your model still achieves high accuracy, and\n",
        "2. implement a train-validate split and assess model overfitting.\n"
      ]
    }
  ]
}